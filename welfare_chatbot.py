import streamlit as st
from streamlit_chatbox import *
import time
import simplejson as json
import re
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, pipeline
import torch
from langchain.schema import Document
from typing import List
import unicodedata
import os
import streamlit as st
import uuid
from langchain.storage import InMemoryStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers.multi_vector import MultiVectorRetriever
    
# ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° í•¨ìˆ˜
def remove_emojis_and_enclosed_chars(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì´ëª¨ì§€ì™€ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    return ''.join(
        ch for ch in text
        if not (
            unicodedata.category(ch).startswith('So')
            or (0x1F000 <= ord(ch) <= 0x1FFFF)
            or (0x2460 <= ord(ch) <= 0x24FF)
        )
    )

# ë¬¸ì„œ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_document(text):
    """ë¬¸ì„œì—ì„œ ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´, íƒœê·¸, ì´ëª¨ì§€ ë“±ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    text = remove_emojis_and_enclosed_chars(text)
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'(\*\*|\*|_)', '', text)
    text = re.sub(r'^[-\*\+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\n+', '\n', text).strip()
    return text

# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def process_pages(pages: List[Document]) -> List[Document]:
    """ê° ë¬¸ì„œë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return [Document(page_content=preprocess_document(page.page_content), metadata=page.metadata) for page in pages]

# ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ (ì²« ë²ˆì§¸ ì½”ë“œì—ì„œ ì¶”ê°€)
def add_documents_in_batches(vectorstore, documents, batch_size=1000):
    """ë¬¸ì„œë“¤ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€"""
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        vectorstore.add_documents(batch)
        print(f"ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")

# ë¬¸ì„œ ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
def load_and_process_documents(file_paths: List[str], embedding_model):
    """ì—¬ëŸ¬ PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  MultiVectorRetriever ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    all_documents = []
    
    for file_path in file_paths:
        try:
            loader = PyMuPDFLoader(file_path)
            documents = loader.load()
            all_documents.extend(documents)
        except Exception as e:
            st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_path}): {str(e)}")
            continue
    
    if not all_documents:
        return None, None, None
    
    # ë¬¸ì„œ ì „ì²˜ë¦¬
    processed_data = process_pages(all_documents)
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vectorstore = Chroma(
        collection_name="welfare_chunks",
        embedding_function=embedding_model,
    )
    
    # ë¶€ëª¨ ë¬¸ì„œì˜ ì €ì¥ì†Œ ê³„ì¸µ
    store = InMemoryStore()
    id_key = "doc_id"
    
    # ê²€ìƒ‰ê¸° ìƒì„±
    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        byte_store=store,
        id_key=id_key,
        search_type="similarity_score_threshold",
        search_kwargs={"score_threshold": 0.5}
    )
    
    # Parent/Child ë¬¸ì„œ ë¶„í• 
    parent_text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
    )
    child_text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
    )
    
    # ê° ì›ë³¸ ë¬¸ì„œë¥¼ parentë¡œ ì²˜ë¦¬
    parent_docs = []
    child_docs = []
    
    for doc in processed_data:
        # ê° ë¬¸ì„œë¥¼ parent ì²­í¬ë¡œ ë¶„í• 
        parent_chunks = parent_text_splitter.split_documents([doc])
        
        for parent_chunk in parent_chunks:
            # Parent ë¬¸ì„œì— ê³ ìœ  ID ë¶€ì—¬
            parent_id = str(uuid.uuid4())
            parent_chunk.metadata[id_key] = parent_id
            parent_docs.append(parent_chunk)
            
            # Parent ì²­í¬ë¥¼ childë¡œ ë¶„í• 
            child_chunks = child_text_splitter.split_documents([parent_chunk])
            
            # ê° child ë¬¸ì„œì— parent ID ì—°ê²°
            for child_chunk in child_chunks:
                child_chunk.metadata[id_key] = parent_id
            
            child_docs.extend(child_chunks)
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ child ë¬¸ì„œë§Œ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
    st.info(f"Parent ë¬¸ì„œ ìˆ˜: {len(parent_docs)}, Child ë¬¸ì„œ ìˆ˜: {len(child_docs)}")
    
    print(f"\nChild ë¬¸ì„œë“¤ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€ ì¤‘...")
    add_documents_in_batches(retriever.vectorstore, child_docs, batch_size=1000)
    
    # Parent ë¬¸ì„œë¥¼ docstoreì— ì €ì¥
    parent_doc_ids = [doc.metadata[id_key] for doc in parent_docs]
    retriever.docstore.mset(list(zip(parent_doc_ids, parent_docs)))
    print("Parent ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
    
    return retriever, len(child_docs), parent_docs

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ìºì‹œ í•¨ìˆ˜
@st.cache_resource
def load_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    embedding_model = HuggingFaceEmbeddings(
        model_name='jhgan/ko-sroberta-multitask',
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True},
    )
    return embedding_model

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - MultiVectorRetrieverë¡œ ëŒ€ì²´)
def create_vectorstore(texts, embedding_model):
    """í…ìŠ¤íŠ¸ ì²­í¬ì™€ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not texts:
        return None
    db = Chroma.from_documents(texts, embedding=embedding_model)
    return db

# LLM ëª¨ë¸ ë¡œë“œ ìºì‹œ í•¨ìˆ˜ 
@st.cache_resource
def load_llm_model():
    """LLM ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    model_name = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    llm_pipeline = pipeline(
        "text-generation",
        model=model_name,
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map="auto",
        max_new_tokens=768,
        temperature=0,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        repetition_penalty=1.1,
    )
    llm = HuggingFacePipeline(pipeline=llm_pipeline)
    return llm

# ê³µí†µ ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜
def _process_query(question, age=None, gender=None, location=None, income=None, family_size=None, marriage=None, children=None, basic_living=None, employment_status=None, pregnancy_status=None, nationality=None, disability=None, military_service=None):    
    """ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  í”„ë¡¬í”„íŠ¸ì™€ ì¶œì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if st.session_state.get("retriever") is None:
        return None, "PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", [], []
    
    # clean_text í•¨ìˆ˜ ì •ì˜
    def clean_text(text):
        text = re.sub(r'<[^>]+>', '', text)
        return re.sub(r'\s+', ' ', text.strip())

    # MultiVectorRetriever ì‚¬ìš© (k=5ê°œ ê²€ìƒ‰)
    try:
        docs = st.session_state.retriever.invoke(question, k=5)
    except Exception as e:
        return None, f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", [], []

    if not docs:
        return None, "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ í‘œí˜„ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.", [], []

    # ë¬¸ì„œ í’ˆì§ˆ í•„í„°ë§
    quality_docs = []
    for doc in docs:
        cleaned_content = clean_text(doc.page_content)
        if len(cleaned_content.strip()) > 100:
            quality_docs.append(doc)

    if not quality_docs:
        return None, "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆì´ ë‚®ì•„ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.", [], []

    # ì°¸ê³ ìë£Œì™€ ì¶œì²˜ ì •ë¦¬
    context_parts = []
    sources = []
    search_results = []  # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ë³€ìˆ˜
    
    for i, doc in enumerate(quality_docs[:3]):
        clean_content = clean_text(doc.page_content)
        context_parts.append(f"[ì°¸ê³ ìë£Œ {i+1}]\n{clean_content[:128]}")
        
        # í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
        page_num = doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')
        source_file = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        
        # ì¶œì²˜ ì •ë³´ ìƒì„±
        if page_num != 'ì•Œ ìˆ˜ ì—†ìŒ':
            source_info = f"ğŸ“„ í˜ì´ì§€ {page_num}"
            if source_file != 'ì•Œ ìˆ˜ ì—†ìŒ':
                source_info += f" ({source_file})"
        else:
            source_info = f"ğŸ“„ {source_file}"
        
        sources.append(source_info)
        
        # ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ (ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìš©ë„)
        search_results.append({
            'content': clean_content[:500] + "..." if len(clean_content) > 500 else clean_content,
            'page': page_num,
            'source': source_file
        })

    context = "\n\n".join(context_parts)

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    user_info = []
    # "í•´ë‹¹ ì—†ìŒ"ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
    if age is not None and str(age).strip() != "":
        user_info.append(f"ë‚˜ì´: {age}")
    if gender is not None:
        user_info.append(f"ì„±ë³„: {gender}")
    if location is not None:
        user_info.append(f"ê±°ì£¼ì§€: {location}")
    if income is not None and str(income).strip() != "":
        user_info.append(f"ì†Œë“: ì¤‘ìœ„ {income}%")
    if family_size is not None:
        user_info.append(f"ê°€êµ¬ í˜•íƒœ: {family_size}")
    if marriage is not None:
        user_info.append(f"ê²°í˜¼ ìœ ë¬´: {marriage}")
    if children is not None and children != "í•´ë‹¹ ì—†ìŒ":
        user_info.append(f"ìë…€ ìˆ˜: {children}ëª…")
    if basic_living is not None and basic_living != "í•´ë‹¹ ì—†ìŒ":
        user_info.append(f"ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ ì—¬ë¶€: {basic_living}")
    if employment_status is not None and employment_status != "í•´ë‹¹ ì—†ìŒ":
        user_info.append(f"ì·¨ì—… ì—¬ë¶€: {employment_status}")
    if pregnancy_status is not None and pregnancy_status != "í•´ë‹¹ ì—†ìŒ":
        user_info.append(f"ì„ì‹ /ì¶œì‚° ìƒíƒœ: {pregnancy_status}")
    if nationality is not None and nationality != "í•´ë‹¹ ì—†ìŒ":
        user_info.append(f"êµ­ì : {nationality}")
    if disability is not None and disability != "í•´ë‹¹ ì—†ìŒ":
        user_info.append(f"ì¥ì•  ìœ ë¬´: {disability}")
    if military_service is not None and military_service != "í•´ë‹¹ ì—†ìŒ":
        user_info.append(f"êµ° ë³µë¬´ ì—¬ë¶€: {military_service}")

    user_info_str = "\n".join(user_info)

    prompt = f"""í•œêµ­ ë³µì§€ì •ì±… ì „ë¬¸ê°€ë¡œì„œ, ì•„ë˜ ì‚¬ìš©ì ì •ë³´ì™€ ì°¸ê³ ìë£Œ, ì¤‘ìš” ì§€ì¹¨ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì•Œê¸° ì‰½ê³  ì •í™•í•˜ê²Œ ë³µì§€ ì •ì±…ì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”.

ë§Œì•½ ë¬¸ì„œì— ë‹µì´ ì—†ê±°ë‚˜ ë¶ˆì™„ì „í•˜ë‹¤ë©´, 'ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤: [ë¶€ì¡±í•œ ì •ë³´ ìš”ì•½]'ì´ë¼ê³  ëª…ì‹œí•´ ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]:
{question}

[ì°¸ê³ ìë£Œ]:
{context}

[ë‹µë³€ ì¤‘ìš” ì§€ì¹¨]:
1. ìµœëŒ€ 3ê°œ ì •ì±…ì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”.  
2. ê° ì •ì±…ë§ˆë‹¤ ì•„ë˜ 6ê°œ í•­ëª©ì„ ëª¨ë‘ ì‘ì„±í•´ ì£¼ì„¸ìš”.  
3. ë‹µë³€ ë§ˆì§€ë§‰ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•´ ì£¼ì„¸ìš”.

[í•„ìˆ˜ í˜•ì‹]:
### ì •ì±… [ë²ˆí˜¸]: [ì •ì±…ëª…]
- ìš”ì•½: [ìš”ì•½ ë‚´ìš©]
- ëŒ€ìƒ: [ëŒ€ìƒ ë‚´ìš©]
- ì§€ì›: [ì§€ì› ë‚´ìš©]
- ë°©ë²•: [ë°©ë²• ë‚´ìš©]
- ì£¼ì˜: [ì£¼ì˜ ë‚´ìš©]
- ë¬¸ì˜: [ë¬¸ì˜ ë‚´ìš©]

[ì •ë³´ ë¶€ì¡± ì‹œ]:  
ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤: [ì´ìœ  ë˜ëŠ” ë¶€ì¡±í•œ ë¶€ë¶„ ìš”ì•½]
ë‹µë³€:"""

    return prompt, None, sources, search_results

# ë‹µë³€ì—ì„œ í”„ë¡¬í”„íŠ¸ ì œê±° í•¨ìˆ˜
def _extract_answer_only(response):
    """LLM ì‘ë‹µì—ì„œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not response:
        return response
    
    # "ë‹µë³€" í‚¤ì›Œë“œ ì´í›„ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
    answer_markers = ["ë‹µë³€:", "ë‹µë³€", "Answer:", "Answer"]
    
    for marker in answer_markers:
        if marker in response:
            parts = response.split(marker, 1)
            if len(parts) > 1:
                return parts[1].strip()
    
    # ë‹µë³€ ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ### ì •ì±…ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ë¶€í„° ì¶”ì¶œ
    lines = response.split('\n')
    answer_lines = []
    start_found = False
    
    for line in lines:
        if line.strip().startswith('### ì •ì±…'):
            start_found = True
        if start_found:
            answer_lines.append(line)
    
    if answer_lines:
        return '\n'.join(answer_lines)
    
    return response

# ì¼ë°˜ ëª¨ë“œ ë‹µë³€ ìƒì„± í•¨ìˆ˜
def generate_answer(question, age=None, gender=None, location=None, income=None, family_size=None, marriage=None, children=None, basic_living=None, employment_status=None, pregnancy_status=None, nationality=None, disability=None, military_service=None):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ (ì¼ë°˜ ëª¨ë“œ)."""
    try:
        result = _process_query(question, age, gender, location, income, family_size, marriage, children, basic_living, employment_status, pregnancy_status, nationality, disability, military_service)
        
        if len(result) == 4:
            prompt, error_msg, sources, search_results = result
        else:
            # ì´ì „ í˜•ì‹ ì§€ì› (3ê°œ ë°˜í™˜ê°’)
            prompt, error_msg, sources = result
            search_results = []
        
        if error_msg:
            return error_msg, [], []
        
        # LLM ì‘ë‹µ ìƒì„±
        try:
            response = st.session_state.llm.predict(prompt)
            if response is None:
                return "ëª¨ë¸ì—ì„œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", [], []
            
            # ë‹µë³€ì—ì„œ í”„ë¡¬í”„íŠ¸ ì œê±°
            clean_response = _extract_answer_only(response)
            return clean_response, sources, search_results
            
        except Exception as e:
            return f"LLM ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [], []

    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [], []

# ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ë‹µë³€ ìƒì„± í•¨ìˆ˜
def generate_answer_streaming(question, age=None, gender=None, location=None, income=None, family_size=None, marriage=None, children=None, basic_living=None, employment_status=None, pregnancy_status=None, nationality=None, disability=None, military_service=None):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)."""
    try:
        result = _process_query(question, age, gender, location, income, family_size, marriage, children, basic_living, employment_status, pregnancy_status, nationality, disability, military_service)
        
        if len(result) == 4:
            prompt, error_msg, sources, search_results = result
        else:
            # ì´ì „ í˜•ì‹ ì§€ì› (3ê°œ ë°˜í™˜ê°’)
            prompt, error_msg, sources = result
            search_results = []
        
        if error_msg:
            yield error_msg, [], []
            return
        
        # LLM ì‘ë‹µ ìƒì„±
        try:
            response = st.session_state.llm.predict(prompt)
            if response is None:
                yield "ëª¨ë¸ì—ì„œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", [], []
                return
            
            # ë‹µë³€ì—ì„œ í”„ë¡¬í”„íŠ¸ ì œê±°
            clean_response = _extract_answer_only(response)
            
            clean_response = remove_emojis_and_enclosed_chars(clean_response)
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
            words = clean_response.split()
            for i in range(0, len(words), 5):  # 5ë‹¨ì–´ì”© ì¶œë ¥
                yield " ".join(words[:i+5]), sources, search_results
                time.sleep(0.1)
                
        except Exception as e:
            yield f"LLM ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [], []

    except Exception as e:
        yield f"ë‹µë³€ ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [], []

# í”¼ë“œë°± ì²˜ë¦¬ í•¨ìˆ˜
def on_feedback(feedback, chat_history_id: str = "", history_index: int = -1):
    """í”¼ë“œë°±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    reason = feedback.get("text", "")
    score = feedback.get("score", 0)
    
    # í”¼ë“œë°± ì €ì¥ ë¡œì§ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥)
    if "feedback_history" not in st.session_state:
        st.session_state.feedback_history = []
    
    st.session_state.feedback_history.append({
        "chat_history_id": chat_history_id,
        "history_index": history_index,
        "score": score,
        "reason": reason,
        "timestamp": time.time()
    })
    
    st.session_state["need_rerun"] = True

# ì±„íŒ… ì„¸ì…˜ ë³€ê²½ í•¨ìˆ˜
def on_chat_change():
    """ì±„íŒ… ì„¸ì…˜ì´ ë³€ê²½ë  ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤."""
    st.session_state.chat_box.use_chat_name(st.session_state["chat_name"])
    st.session_state.chat_box.context_to_session()

# ì¶”ê°€ ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜
def load_additional_documents(uploaded_files):
    """ì¶”ê°€ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    pdf_dir = "./pdf/welfare"
    
    with st.spinner("ì¶”ê°€ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
            additional_files = []
            for uploaded_file in uploaded_files:
                with open(uploaded_file.name, "wb") as f:
                    f.write(uploaded_file.getvalue())
                additional_files.append(uploaded_file.name)
            
            # ê¸°ë³¸ ë¬¸ì„œì™€ ì¶”ê°€ ë¬¸ì„œ í•©ì¹˜ê¸°
            all_files = []
            
            # ê¸°ë³¸ ë¬¸ì„œ ì¶”ê°€
            if os.path.exists(pdf_dir):
                pdf_files = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
                all_files.extend(pdf_files)
            
            # ì¶”ê°€ ë¬¸ì„œ ì¶”ê°€
            all_files.extend(additional_files)
            
            st.info(f"ê¸°ë³¸ ë¬¸ì„œ í¬í•¨ ì´ {len(all_files)}ê°œ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")

            # MultiVectorRetriever ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬
            result = load_and_process_documents(all_files, st.session_state.embedding_model)
            
            if result[0] is not None:  # retrieverê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                retriever, total_chunks, processed_docs = result
                st.session_state.retriever = retriever
                st.session_state.processed_docs = processed_docs
                
                st.success(f"âœ… ì¶”ê°€ ë¬¸ì„œ í¬í•¨ ì´ {total_chunks}ê°œ ë¬¸ì„œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.session_state.documents_loaded = True
                return True
            else:
                st.error("ì¶”ê°€ ë¬¸ì„œ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            st.error(f"ì¶”ê°€ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False

# ê¸°ë³¸ ë¬¸ì„œ ìë™ ë¡œë“œ í•¨ìˆ˜ (MultiVectorRetriever ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
def load_default_documents():
    """í˜ì´ì§€ ì‹œì‘ ì‹œ ê¸°ë³¸ ë³µì§€ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    if st.session_state.get("default_documents_loaded", False):
        return
    
    pdf_dir = "./pdf/welfare"
    
    if os.path.exists(pdf_dir):
        pdf_files = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
        
        if pdf_files:
            try:
                with st.spinner("ê¸°ë³¸ ë³µì§€ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
                    if "embedding_model" not in st.session_state:
                        st.session_state.embedding_model = load_embedding_model()
                    
                    # MultiVectorRetriever ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬
                    result = load_and_process_documents(pdf_files, st.session_state.embedding_model)
                    
                    if result[0] is not None:  # retrieverê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        retriever, total_chunks, processed_docs = result
                        st.session_state.retriever = retriever
                        st.session_state.processed_docs = processed_docs
                        
                        # LLM ëª¨ë¸ ë¡œë“œ
                        if "llm" not in st.session_state:
                            st.session_state.llm = load_llm_model()
                        
                        st.session_state.default_documents_loaded = True
                        st.session_state.documents_loaded = True
                        st.success(f"âœ… ê¸°ë³¸ ë³µì§€ ë¬¸ì„œ {total_chunks}ê°œ ì²­í¬ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
            except Exception as e:
                st.error(f"ê¸°ë³¸ ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ë©”ì¸ ì•±
def main():
    st.set_page_config(
        page_title="ë³µì§€PT",
        page_icon="ğŸ›ï¸",
        layout="wide"
    )
    
    # ChatBox ì´ˆê¸°í™”
    if "chat_box" not in st.session_state:
        st.session_state.chat_box = ChatBox(
            use_rich_markdown=False,
            user_theme="green",
            assistant_theme="blue",
        )
        st.session_state.chat_box.use_chat_name("welfare_chat")
    
    chat_box = st.session_state.chat_box
    
    # ê¸°ë³¸ ë¬¸ì„œ ìë™ ë¡œë“œ
    load_default_documents()
    
    # ì‚¬ì´ë“œë°” êµ¬ì„±
    with st.sidebar:        
        # ì±„íŒ… ì„¸ì…˜ ì„ íƒ
        chat_name = st.selectbox(
            "ì±„íŒ… ì„¸ì…˜:", 
            ["welfare_chat", "general_chat"], 
            key="chat_name", 
            on_change=on_chat_change
        )
        chat_box.use_chat_name(chat_name)
        
        # ì„¤ì • ì˜µì…˜
        streaming = st.checkbox('ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ', key="streaming")
        show_history = st.checkbox('ì„¸ì…˜ ìƒíƒœ ë³´ê¸°', key="show_history")
        
        chat_box.context_from_session(exclude=["chat_name"])
        
        st.divider()
        
        # ì‚¬ìš©ì ì •ë³´ ì…ë ¥
        st.subheader("ì‚¬ìš©ì ì •ë³´")
        age = st.text_input("ë‚˜ì´", value="", placeholder="ë‚˜ì´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        gender = st.radio("ì„±ë³„", options=["ë‚¨", "ì—¬"], index=0)
        family_size = ["í•´ë‹¹ ì—†ìŒ", "1ì¸ ê°€êµ¬", "í•œë¶€ëª¨ê°€ì¡±", "ë‹¤ìë…€ê°€ì •"]
        family_size = st.selectbox("ê°€êµ¬ í˜•íƒœ", options=family_size, index=0)
        
        # ê²°í˜¼ ìœ ë¬´ ì…ë ¥
        marriage = ["í•´ë‹¹ ì—†ìŒ", "ê¸°í˜¼"]
        marriage = st.selectbox("ê²°í˜¼ ìœ ë¬´", options=marriage, index=0)
        
        # êµ­ì  ì…ë ¥
        nationality = ["í•´ë‹¹ ì—†ìŒ", "ì™¸êµ­ì¸", "ì¬ì™¸êµ­ë¯¼", "ë‚œë¯¼"]
        nationality = st.selectbox("êµ­ì ", options=nationality, index=0)
        
        # ì¥ì•  ìœ ë¬´ ì…ë ¥
        disability = st.radio("ì¥ì•  ìœ ë¬´", options=["í•´ë‹¹ ì—†ìŒ", "ìˆìŒ"], index=0)
        
        # ë³‘ì—­ ìœ ë¬´ ì…ë ¥
        military_service = ["í•´ë‹¹ ì—†ìŒ", "êµ°í•„", "ë³µë¬´ ì¤‘"]
        military_service = st.selectbox("ë³‘ì—­ ìœ ë¬´", options=military_service, index=0)
        
        # ì·¨ì—… ì—¬ë¶€ (ì‹¤ì§ì/êµ¬ì§ì/ì¬ì§ì)
        employment_status = ["í•´ë‹¹ ì—†ìŒ", "ì¬ì§ì", "ì‹¤ì§ì"]
        employment_status = st.selectbox("ì·¨ì—… ì—¬ë¶€", options=employment_status, index=0)
        
        # ì„ì‹ /ì¶œì‚° ìƒíƒœ (ì„ì‚°ë¶€, ì¶œì‚° í›„ 6ê°œì›” ì´ë‚´, í•´ë‹¹ ì—†ìŒ)
        pregnancy_status = ["í•´ë‹¹ ì—†ìŒ", "ì„ì‚°ë¶€", "ì¶œì‚° í›„ 6ê°œì›” ì´ë‚´"]
        pregnancy_status = st.selectbox("ì„ì‹ /ì¶œì‚°", options=pregnancy_status, index=0)
        
        # ìë…€ ìˆ˜ ì„ íƒ
        children_options = ["í•´ë‹¹ ì—†ìŒ", "1ëª…", "2ëª…", "3ëª…", "4ëª…", "5ëª…", "6ëª…", "7ëª…", "8ëª…", "9ëª…", "10ëª…"]
        children = st.selectbox("ìë…€ ìˆ˜", options=children_options, index=0)

        # ê±°ì£¼ì§€
        locations = ["ì„œìš¸", "ìˆ˜ì›", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…", 
                    "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼"]
        location = st.selectbox("ê±°ì£¼ì§€", options=locations, index=0)
        
        # ì†Œë“ (ì¤‘ìœ„ %)
        income = st.slider("ì†Œë“ (ì¤‘ìœ„ %)", min_value=10, max_value=90, value=50, step=1)
        
        # ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ ì—¬ë¶€ (ìˆ˜ê¸‰ì/ë¹„ìˆ˜ê¸‰ì)
        basic_living = st.radio("ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ ì—¬ë¶€", options=["í•´ë‹¹ ì—†ìŒ", "ìˆ˜ê¸‰ì"], index=0)
        
        st.divider()
        
        # ë¬¸ì„œ ìƒíƒœ í‘œì‹œ
        st.subheader("ë¬¸ì„œ ìƒíƒœ")
        
        pdf_dir = "./pdf/welfare"
        
        # ê¸°ë³¸ ë¬¸ì„œ ìƒíƒœ í‘œì‹œ
        if st.session_state.get("default_documents_loaded", False):
            if os.path.exists(pdf_dir):
                default_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
                st.success(f"âœ… ê¸°ë³¸ ë³µì§€ ì •ì±… ë¬¸ì„œ {len(default_files)}ê°œ íŒŒì¼ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                with st.expander("ê¸°ë³¸ ë¬¸ì„œ ëª©ë¡ ë³´ê¸°"):
                    for file in default_files:
                        st.write(f"ğŸ“„ {file}")
            else:
                st.warning("âš ï¸ ê¸°ë³¸ ë³µì§€ ì •ì±… ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if os.path.exists(pdf_dir) and len([f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]) > 0:
                st.info("â³ ê¸°ë³¸ ë³µì§€ ì •ì±… ë¬¸ì„œë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")
            else:
                st.warning("âš ï¸ pdf/welfare í´ë”ì— ê¸°ë³¸ ë³µì§€ ì •ì±… ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ ì„¹ì…˜
        st.subheader("ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        uploaded_files = st.file_uploader(
            "ì¶”ê°€ ë³µì§€ ì •ì±… PDF íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì„ íƒì‚¬í•­)",
            type=["pdf"],
            accept_multiple_files=True,
            help="ê¸°ë³¸ ë¬¸ì„œì— ì¶”ê°€ë¡œ ë” ë§ì€ ì •ì±… ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        # ì¶”ê°€ ë¬¸ì„œ ë¡œë“œ ë²„íŠ¼
        if st.button("ì¶”ê°€ ë¬¸ì„œ ë¡œë“œ", type="secondary"):
            if uploaded_files:
                success = load_additional_documents(uploaded_files)
                if success:
                    st.rerun()
            else:
                st.warning("ì—…ë¡œë“œí•  íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        st.divider()
        
        # ë‚´ë³´ë‚´ê¸° ë²„íŠ¼
        btns = st.container()
        
        if btns.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ì—­ ì‚­ì œ"):
            chat_box.init_session(clear=True)
            st.rerun()
    
    # ë©”ì¸ ì±„íŒ… ì˜ì—­
    # ë©‹ì§„ ì²« í™”ë©´ ë¬¸êµ¬ì™€ í°íŠ¸ í¬ê¸° ì¡°ì •
    st.markdown(
        """
        <h1 style='text-align: center; font-size: 3.2em;'>ë³µì§€PTì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!</h1>
        <p style='text-align: center; font-size: 1.5em; color: #555;'>
            ë‹¹ì‹ ì˜ ìƒí™©ì— ê¼­ ë§ëŠ” ë³µì§€ ì •ì±…ì„ <b>AI</b>ê°€ ì‰½ê³  ë¹ ë¥´ê²Œ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.<br>
            ê¶ê¸ˆí•œ ì ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ë³´ì„¸ìš”!
        </p>
        """,
        unsafe_allow_html=True
    )
    
    # ì‚¬ìš© íŒ í‘œì‹œ
    if not st.session_state.get("chat_started", False):
        st.info("""
        ğŸ’¡ **ì‚¬ìš© íŒ**
        - ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ê°œì¸ì •ë³´ë¥¼ ì…ë ¥í•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ì˜ˆì‹œ: "30ëŒ€ ì‹ í˜¼ë¶€ë¶€ë¥¼ ìœ„í•œ ì£¼ê±° ì§€ì› ì •ì±…ì„ ì•Œë ¤ì£¼ì„¸ìš”"
        - ì±„íŒ… í›„ ì•„ë˜ ì‚¬ìš©ì„¤ëª…ì„œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!
        - ì¶”ê°€ì ì¸ ë¬¸ì„œ(PDF)ë¥¼ ì—…ë¡œë“œí•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)
        """)
    
    # ì±„íŒ…ì´ ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if len(st.session_state.chat_box.history) > 0:
        st.session_state.chat_started = True
    
    # ì±„íŒ… ë°•ìŠ¤ ì´ˆê¸°í™” ë° ì¶œë ¥
    chat_box.init_session()
    chat_box.output_messages()
    
    # í”¼ë“œë°± ì„¤ì •
    feedback_kwargs = {
        "feedback_type": "thumbs",
        "optional_text_label": "í”¼ë“œë°±ì„ ë‚¨ê²¨ì£¼ì„¸ìš”",
    }
    
    # ì±„íŒ… ì…ë ¥ ì²˜ë¦¬
    if query := st.chat_input('ë‚˜ì—ê²Œ ì•Œë§ëŠ” ë³µì§€ í˜œíƒ ì•Œë ¤ì£¼ì„¸ìš”.'):
        # ê¸°ë³¸ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not st.session_state.get("default_documents_loaded", False):
            st.error("â³ ê¸°ë³¸ ë³µì§€ ì •ì±… ë¬¸ì„œë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!")
            return
        

        chat_box.user_say(query)
        
        age_val = age if age.strip() else None
        
        if streaming:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
            try:
                # ë‹µë³€ ë° ì°¸ê³ ìë£Œ ì˜ì—­ì„ í…ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                elements = chat_box.ai_say([
                    "ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                    "",
                ])
                
                generator = generate_answer_streaming(
                    question=query,
                    age=age_val,
                    gender=gender,
                    location=location,
                    income=income,
                    family_size=family_size,
                    marriage=marriage,
                    children=children,
                    employment_status=employment_status,
                    pregnancy_status=pregnancy_status,
                    nationality=nationality,
                    disability=disability,
                    military_service=military_service,
                    basic_living=basic_living
                )
                
                text = ""
                sources = []
                search_results = []
                try:
                    for response, doc_sources, doc_search_results in generator:
                        text = response
                        sources = doc_sources
                        search_results = doc_search_results
                        chat_box.update_msg(text, element_index=0, streaming=True)
                except Exception as e:
                    text = f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    sources = []
                    search_results = []
                
                chat_box.update_msg(text, element_index=0, streaming=False, state="complete")
                
                # ê²€ìƒ‰ ê²°ê³¼ì™€ ì°¸ê³ ìë£Œ í‘œì‹œ
                reference_text = ""
                if search_results:
                    reference_text += "ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´:\n\n"
                    for i, result in enumerate(search_results, 1):
                        reference_text += f"{i}. {os.path.basename(result['source'])} (í˜ì´ì§€ {result['page']})\n"
                        reference_text += f"{result['content']}\n\n"
                    reference_text += "---\n\n"
                
                if sources:
                    reference_text += "ì°¸ê³ ìë£Œ:\n" + "\n".join(sources)
                else:
                    reference_text += "ì°¸ê³ ìë£Œ: ì—†ìŒ"
                
                chat_box.update_msg(reference_text, element_index=1, streaming=False, state="complete")
                
                # í”¼ë“œë°± í‘œì‹œ
                chat_history_id = f"chat_{len(chat_box.history)}"
                chat_box.show_feedback(
                    **feedback_kwargs,
                    key=chat_history_id,
                    on_submit=on_feedback,
                    kwargs={"chat_history_id": chat_history_id, "history_index": len(chat_box.history) - 1}
                )
            except Exception as e:
                chat_box.ai_say([
                    f"ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì˜¤ë¥˜: {str(e)}",
                    "ğŸ“„ ì°¸ê³ ìë£Œ: ì—†ìŒ",
                ])
        else:
            # ì¼ë°˜ ëª¨ë“œ
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    result = generate_answer(
                        question=query,
                        age=age_val,
                        gender=gender,
                        location=location,
                        income=income,
                        family_size=family_size,
                        basic_living=basic_living,
                        marriage=marriage,
                        children=children,
                        employment_status=employment_status,
                        pregnancy_status=pregnancy_status,
                        nationality=nationality,
                        disability=disability,
                        military_service=military_service
                    )
                    # ë°˜í™˜ê°’ì´ tupleì¸ì§€ í™•ì¸
                    if isinstance(result, tuple) and len(result) == 3:
                        text, sources, search_results = result
                    else:
                        text = str(result)
                        sources = []
                        search_results = []

                    
                except Exception as e:
                    text = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    sources = []
                    search_results = []
            
            reference_text = ""
            reference_text += "---\n\n"            
            if search_results:
                reference_text += "ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´:\n\n"
                for i, result in enumerate(search_results, 1):
                    reference_text += f"{i}. {os.path.basename(result['source'])} (í˜ì´ì§€ {result['page']})\n"
                    reference_text += f"{result['content']}\n\n"
            
            # ë‹µë³€ê³¼ ì°¸ê³ ìë£Œë¥¼ í…ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤. (Markdownì´ ì•„ë‹Œ plain text)
            chat_box.ai_say([
                text,
                reference_text,
            ])
    
    # ì„¸ì…˜ ìƒíƒœ ë³´ê¸°
    if show_history:
        st.subheader("ì„¸ì…˜ ìƒíƒœ")
        st.write(st.session_state)

if __name__ == "__main__":
    main() 